Log Formats
http://httpd.apache.org/docs/2.2/logs.html
https://httpd.apache.org/docs/current/mod/mod_log_config.html#formats

"%h %l %u %t \"%r\" %>s %b \"%{Referer}i\" \"%{User-agent}i\""


Troubleshoot with Apache Logs
There are several ways for a website to break, and when they do, you’ll want to have the information needed to quickly bring it back online. If you or your users experience an error, you may want to find out the following.

What page the user was on
What the status code was (i.e., 404)
How much load the server was under at the time
How long it took the page to load
Any unusual traffic patterns
What browser (or user agent) the user had
how many threads 
how much cpu , memory & lsof

https://www.loggly.com/ultimate-guide/apache-logging-basics/

Access log contains information about requests coming in to the web server. 
This information can include what pages people are viewing, the success status of requests, and how long the server took to respond.

Error log contains information about errors the web server encountered when processing requests, such as missing files. It also includes diagnostic information about the server itself.

status codes:
Below extracts patterns of three digits surrounded by spaces.
grep -o " [0-9]{3} " /var/log/apache2/access.log

$ cat access.log | cut -d ' ' -f 9


Are There Too Many Errors?
Apache logs have errors from two sources: the error log and error status codes in the access log. HTTP status codes that are errors are 400 or above
$ cat access.log | cut -d ' ' -f 9 | sort | uniq -c | sort -nr
     941 200
     292 500
     290 404
     50 401
     20 400
     
What is Causing 404s?
A 404 error is defined as a missing file or resource. Looking at the request URI will tell you which one it is.
$ grep " 404 " access.log | cut -d ' ' -f 7 | sort | uniq -c | sort -nr
     17 /manager/html
      3 /robots.txt
      3 /phpMyAdmin/scripts/setup.php
      3 //myadmin/scripts/setup.php
      3 /favicon.ico
      
Breakdown of errors by browser.
filter your logs on a specific status code (404 or 500, for example) by the userAgent field. 
If you are running an API, it’s helpful to see which client libraries may have issues. 
The Apache logs can aid in troubleshooting issues with client libraries or agents, and even show you which are most popular. 
$ cat access.log.1 | cut -d '"' -f 6 | sort | uniq -c | sort -nr
     51 -
     47 ZmEu
     31 () { :;};/usr/bin/perl -e 'print 
     14 Apache/2.2.22 (Ubuntu) (internal dummy connection)
     11 Mozilla/5.0
     
Site Loading Too Slowly?
extract the request time field, and then use a tool like awk to calculate the average. 
In this example, the average is 362k microseconds, or 0.362 seconds.
 ubuntu@ip-172-31-11-241:/var/log/apache2$ cat access.log | cut -d ' ' -f 1 | sort | uniq -c | sort -nr
     31 91.142.209.68
     29 94.102.49.11
     28 173.236.125.26
     19 222.74.212.77
     14 127.0.0.1
362935


Too Much Load From One Source?
A configuration or system problem
A client app or bot hitting your site too fast
A denial of service attack

To find an answer from your Apache fields.
The useragent field can give you a hint as to whether an app or bot is hitting your site. Many simple bots label themselves as such.
The remoteaddr field can tell you if specific IPs are generating a significant proportion of traffic.

 ubuntu@ip-172-31-11-241:/var/log/apache2$ cat access.log.1 | cut -d ' ' -f 1 | sort | uniq -c | sort -nr
     31 91.142.219.68
     29 94.102.49.11
     28 173.236.125.26
     19 222.74.212.77
     14 127.0.0.1
 
 
Unusual Traffic Patterns?
You’ll want to keep a monitor up and running on your site to look for unusual behavior that could be a security problem, or even potential attacks.
There are two big approaches to finding unusual events: top-down and bottom-up. 

For top-down, look at high level metrics to see if there are unusual traffic patterns that could compromise your site (such as too many requests from a particular IP). 
You can watch these on a dashboard or set alerts when critical thresholds are reached.

For a bottom-up analysis, start by subtracting the legitimate traffic you already know about. Drill down to just the errors. Then within the errors, look at each one to determine the cause. 
Oftentimes, 80% of the errors are caused by a small number of known problems, so subtract those from your search. Now you can easily see unusual things like odd user agents or URLs that aren’t legitimate

     
syslog and rsyslog https://www.rsyslog.com/
No logging to disk it pipes directly to syslog
ErrorLog  "| /usr/bin/logger -thttpd -plocal6.err"
CustomLog "| /usr/bin/logger -thttpd -plocal6.notice" extended_ncsa

writing to logs as well as syslog
ErrorLog  "|/usr/bin/tee -a /var/log/www/error.log  | /usr/bin/logger -thttpd -plocal6.err"
CustomLog "|/usr/bin/tee -a /var/log/www/access.log | /usr/bin/logger -thttpd -plocal6.notice" extended_ncsa

The most common HTTP responses are: 
https://www.the-art-of-web.com/system/logstatus/
https://www.w3.org/Protocols/rfc2616/rfc2616-sec10.html
200 - OK
206 - Partial Content
301 - Moved Permanently
302 - Found
304 - Not Modified
401 - Unauthorised (password required)
403 - Forbidden
404 - Not Found

https://www.the-art-of-web.com/system/logs/

Using the default separator which is any white-space (spaces or tabs) we get the following:
awk '{print $1}' combined_log         # ip address (%h)
awk '{print $2}' combined_log         # RFC 1413 identity (%l)
awk '{print $3}' combined_log         # userid (%u)
awk '{print $4,5}' combined_log       # date/time (%t)
awk '{print $9}' combined_log         # status code (%>s)
awk '{print $10}' combined_log        # size (%b)

You might notice that we've missed out some items. To get to them we need to set the delimiter to the " character which changes the way the lines are 'exploded' and allows the following:

awk -F\" '{print $2}' combined_log    # request line (%r)
awk -F\" '{print $4}' combined_log    # referer
awk -F\" '{print $6}' combined_log    # user agent


 list all user agents ordered by the number of times they appear (descending order):
 awk -F\" '{print $6}' combined_log | sort | uniq -c | sort -fr
 
If you're not particulary interested in which operating system the visitor is using, or what browser extensions they have, then you can use something like the following:
awk -F\" '{print $6}' combined_log \
  | sed 's/(\([^;]\+; [^;]\+\)[^)]*)/(\1)/' \
  | sort | uniq -c | sort -fr
This will strip out the third and subsequent values in the 'bracketed' component of the user agent string. For example:
	Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1; .NET CLR)
becomes:
	Mozilla/4.0 (compatible; MSIE 6.0) 
 
Would you like to know which pages Google has been requesting from your site?
awk -F\" '($6 ~ /Googlebot/){print $2}' combined_log | awk '{print $2}'
Or who's been looking at your guestbook?
awk -F\" '($2 ~ /guestbook\.html/){print $6}' combined_log

identifying the different server responses and the requests that caused them:
awk '{print $9}' combined_log | sort | uniq -c | sort

A 404 code may indicate that you have a problem - a broken internal link or someone linking to a page that no longer exists. 
You might need to fix the link, contact the site with the broken link And rewrite url with permanent redirect URL
# list all 404 requests
awk '($9 ~ /404/)' combined_log

# summarise 404 requests
awk '($9 ~ /404/)' combined_log | awk '{print $9,$7}' | sort

use an inverted regular expression to summarise the requests that didn't return 200 ("OK"):
awk '($9 !~ /200/)' combined_log | awk '{print $9,$7}' | sort | uniq

you can include (or exclude in this case) a range of responses, in this case requests that returned 200 ("OK") or 304 ("Not Modified"):
awk '($9 !~ /200|304/)' combined_log | awk '{print $9,$7}' | sort | uniq

Suppose you've identifed a link that's generating a lot of 404 errors. Let's see where the requests are coming from:

awk -F\" '($2 ~ "^GET /path/to/brokenlink\.html"){print $4,$6}' combined_log
Now you can see not just the referer, but the user-agent making the request. You should be able to identify whether there is a broken link within your site, on an external site, or if a search engine or similar agent has an invalid address.
If you can't fix the link, you should look at using Apache mod_rewrite or a similar scheme to redirect (301) the requests to the most appropriate page on your site. By using a 301 instead of a normal (302) redirect you are indicating to search engines and other intelligent agents that they need to update their link as the content has 'Moved Permanently'.


Blank User Agents
A 'blank' user agent is typically an indication that the request is from an automated script or someone who really values their privacy. The following command will give you a list of ip addresses for those user agents so you can decide if any need to be blocked:
awk -F\" '($6 ~ /^-?$/)' combined_log | awk '{print $1}' | sort | uniq
A further pipe through logresolve will give you the hostnames of those addresses.

# list image requests that returned 403 Forbidden after blocking them
awk '($9 ~ /403/)' combined_log \
 | awk -F\" '($2 ~ /\.(jpg|gif)/){print $4}' \
 | sort | uniq -c | sort

or 

awk -F\" '{split($3,ar," "); if(ar[1] == "403")print $0}' combined.log


tail -n 2 access.log error.log
tail -f access.log | grep 404
tail access.log | grep -A 2 "GET \/bwapp\/"
tail access.log | grep -B 2 "POST \/wordpress"

https://phoenixnap.com/kb/prevent-brute-force-attacks
https://www.the-art-of-web.com/system/rewrite/
